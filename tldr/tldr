#!/usr/bin/env python

import os
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'

import sys
import pysam
import subprocess
import argparse
import pickle
import multiprocessing as mp
import numpy as np
import gc

from uuid import uuid4
from collections import defaultdict as dd
from collections import OrderedDict as od
from collections import Counter
from bx.intervals.intersection import Intersecter, Interval
from tqdm import tqdm

from sklearn import metrics
from sklearn.mixture import GaussianMixture

import logging
FORMAT = '%(asctime)s %(message)s'
logging.basicConfig(format=FORMAT)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

import random
random.seed(1)


class AlignedColumn:
    def __init__(self):
        self.bases = od() # species name --> base
        self.annotations = od() # metadata

    def gap(self):
        if '-' in self.bases.values():
            return True
        return False

    def subst(self):
        if self.gap():
            return False

        if len(set(self.bases.values())) > 1:
            return True
        return False

    def cons(self, min_override_gap=2):
        ''' weighted consensus '''
        most_common = Counter(map(str.upper, self.bases.values())).most_common()[0]

        if most_common[0] != '-':
            return most_common

        if len(Counter(map(str.upper, self.bases.values())).most_common()) > 1:
            if Counter(map(str.upper, self.bases.values())).most_common()[1][1] >= min_override_gap:
                return Counter(map(str.upper, self.bases.values())).most_common()[1]

        return most_common

    def __str__(self):
        return str(self.bases)


class MSA:
    ''' multiple sequence alignment class '''
    def __init__(self, infile=None):
        self.columns = []
        self.ids     = []
        self.seqs    = od()

        if infile is not None:
            self.readFastaMSA(infile)

    def __len__(self):
        return len(self.columns)

    def readFastaMSA(self, infile):
        id   = None
        seq  = ''

        with open(infile, 'r') as fasta:
            for line in fasta:
                line = line.strip()
                if line.startswith('>'):
                    if id is not None:
                        self.seqs[id] = seq
                    seq = ''
                    id = line.lstrip('>')
                    self.ids.append(id)
                else:
                    seq += line
            self.seqs[id] = seq

        first = True
        colen = 0
        for ID, seq in self.seqs.items():
            if first:
                colen = len(seq)
                for base in list(seq):
                    ac = AlignedColumn()
                    ac.bases[ID] = base
                    self.columns.append(ac)
                first = False
            else:
                assert len(seq) == colen
                pos = 0
                for base in list(seq):
                    ac = self.columns[pos]
                    ac.bases[ID] = base
                    pos += 1

    def consensus(self, gapped=False):
        ''' compute consensus '''
        bases = []
        for column in self.columns:
            b = column.cons()[0]
            if gapped or b != '-':
                bases.append(b)
        return ''.join(bases)


class InsRead:
    def __init__(self, bamfile, q_chrom, q_start, q_end, r_start, r_end, r_name, r_seq, r_qual, r_mapq, is_ins, is_clip, clip_end, phase):
        self.bampath  = bamfile
        self.bamname  = '.'.join(os.path.basename(bamfile).split('.')[:-1])
        self.q_chrom  = q_chrom
        self.q_start  = q_start
        self.q_end    = q_end
        self.r_start  = r_start
        self.r_end    = r_end
        self.r_name   = r_name
        self.r_seq    = r_seq
        self.r_qual   = r_qual
        self.r_mapq   = r_mapq
        self.is_ins   = is_ins
        self.is_clip  = is_clip
        self.clip_end = clip_end
        self.phase    = phase

        self.r_seq_trimmed   = None
        self.r_qual_trimmed  = None
        self.q_start_trimmed = None
        self.q_end_trimmed   = None

        # te alignment info
        self.te             = None
        self.te_fam         = None
        self.te_subfam      = None
        self.te_elt_start   = None
        self.te_elt_end     = None
        self.te_r_seq_start = None
        self.te_r_seq_end   = None
        self.te_read_orient = None

        self.useable = False # set by process_cluster

        self.r_pos = None

        if self.r_qual is None:
            self.r_qual = "F"*len(self.r_seq)

        if self.r_start:
            self.r_pos = self.r_start
        else:
            self.r_pos = self.r_end

    def te_info(self, te):
        self.te = te

        subfam = te.subfamily()

        if ':' in subfam:
            self.te_fam, self.te_subfam = subfam.split(':')
        else:
            self.te_fam    = subfam
            self.te_subfam = subfam

        self.te_elt_start   = te.min_qry_coord()
        self.te_elt_end     = te.max_qry_coord()
        self.te_r_seq_start = te.min_tgt_coord()
        self.te_r_seq_end   = te.max_tgt_coord()
        self.te_read_orient = te.max_qry_orient()

    def fastq(self):
        return '@%s\n%s\n+\n%s\n' % (self.r_name, self.r_seq_trimmed, self.r_qual_trimmed)

    def fasta(self):
        type = 'ins'
        if self.is_clip:
            type = 'clip_%s' % self.clip_end

        return '>%s_%s\n%s\n' % (self.r_name, type, self.r_seq_trimmed) 

    def trim(self, flanksize=200):

        trim_start = self.q_start
        trim_end = self.q_end

        if trim_start < flanksize:
            trim_start = 0
            self.q_start_trimmed = self.q_start

        else:
            trim_start -= flanksize
            self.q_start_trimmed = flanksize

        if trim_end + flanksize > len(self.r_seq):
            trim_end = len(self.r_seq)

        else:
            trim_end += flanksize

        self.q_end_trimmed = self.q_start_trimmed + (self.q_end - self.q_start)
        self.r_seq_trimmed = self.r_seq[trim_start:trim_end]
        self.r_qual_trimmed = self.r_qual[trim_start:trim_end]

    def te_overlap_frac(self):
        if None in (self.q_end_trimmed, self.te_r_seq_end, self.q_start_trimmed, self.te_r_seq_start):
            return 0.0

        return float(self.te_r_seq_end - self.te_r_seq_start) / float(self.q_end_trimmed - self.q_start_trimmed)

    def __lt__(self, other):
        return self.r_pos < other.r_pos


class InsCluster:
    def __init__(self, uuid):
        self.reads  = []
        self.uuid   = uuid
        self.cons   = None

        self.cons_te_align = None
        self.cons_align_map = None

        self.reset_major_family = None
        self.reset_major_subfam = None

        self.breakpoints = []
        self.breakpoints_remappable = False
        self.tsd_seq = 'NA'
        self.empty_reads = dd(list)

    def add(self, read, flanksize=200):
        self.reads.append(read)

    def downsample(self, target_size):
        assert self.cons is None, 'cannot downsample a processed cluster'

        ds_cluster = InsCluster(self.uuid)
        
        ins_reads = [r for r in self.reads if r.is_ins]
        clip_reads = [r for r in self.reads if r.is_clip]

        for read in ins_reads:
            if len(ds_cluster) < target_size:
                ds_cluster.add(read)
            else:
                return ds_cluster

        for read in clip_reads:
            if len(ds_cluster) < target_size:
                ds_cluster.add(read)
            else:
                return ds_cluster

        return ds_cluster

    def chrom(self):
        return self.reads[0].q_chrom

    def interval(self):
        p = sorted([read.r_pos for read in self.reads if read.useable])
        return p[0], p[-1]

    def dump_fastq(self, te_fam=None, sample=None):
        out_fn = '%s.%s.cluster.fq' % (str(sample), self.uuid)

        with open(out_fn, 'w') as out:
            for read in self.reads:
                if not read.useable:
                    continue

                if sample is not None and read.bamname != sample:
                    continue

                if read.te: # require TE alignment
                    if te_fam:
                        if read.te_fam == te_fam:
                            out.write(read.fastq())
                    else:
                        out.write(read.fastq())

                elif te_fam == 'NR': # no TE ref mode
                    out.write(read.fastq())

        return out_fn

    def dump_fasta(self, te_fam=None):
        out_fn = '%s.cluster.fa' % self.uuid

        with open(out_fn, 'w') as out:
            for read in self.reads:
                if not read.useable:
                    continue

                if read.te: # require TE alignment
                    if te_fam:
                        if read.te_fam == te_fam:
                            out.write(read.fasta())
                    else:
                        out.write(read.fasta())

                elif te_fam == 'NR': # no TE ref mode
                    out.write(read.fasta())


        return out_fn

    def te_align_count(self, family=None):
        if family:
            return len([r for r in self.reads if r.te_fam == family])
        else:
            return len([r for r in self.reads if r.te])

    def te_major_family(self):
        if self.reset_major_family is None:
            return Counter([read.te_fam for read in self.reads]).most_common()[0][0]

        return self.reset_major_family

    def te_major_subfam(self):
        if self.reset_major_subfam is None:
            return Counter([read.te_subfam for read in self.reads]).most_common()[0][0]

        return self.reset_major_subfam

    def te_orientation(self):
        return Counter([read.te_read_orient for read in self.reads]).most_common()[0][0]

    def te_fam_frac(self, family):
        return float(len([r for r in self.reads if r.te_fam == family])) / float(len(self))

    def te_median_overlap(self, span_only=False):
        if span_only:
            return np.median([read.te_overlap_frac() for read in self.reads if read.useable and read.is_ins])

        return np.median([read.te_overlap_frac() for read in self.reads if read.useable])

    def te_median_length(self):
        l = []
        for read in self.reads:
            if read.useable and read.is_ins:
                l.append(read.te_elt_end - read.te_elt_start)

        return np.median(l)

    def te_median_mapq(self):
        return np.median([read.r_mapq for read in self.reads if read.useable])

    def te_useable_count(self):
        return len([read for read in self.reads if read.useable])

    def te_embedded_count(self):
        return len([read for read in self.reads if read.useable and read.is_ins])

    def te_samples(self):
        return len(set([read.bamname for read in self.reads if read.useable]))

    def te_sample_count(self):
        sample_count = Counter([read.bamname for read in self.reads if read.useable])
        return ','.join(['|'.join(map(str, k)) for k in sample_count.items()])

    def te_match(self):
        if self.cons_te_align is None:
            return 0.0
        
        return self.cons_te_align.avg_match()

    def detect_inversion(self):
        if self.cons_te_align is None:
            return False

        return self.cons_te_align.max_qry_orient() != self.cons_te_align.min_qry_orient()

    def spanning_non_supporting_reads(self, wiggle, min_te_len, cov_pct=0.95):
        read_names = set([read.r_name for read in self.reads if read.useable])

        for bampath in set([read.bampath for read in self.reads if read.useable]):
            bamname = '.'.join(os.path.basename(bampath).split('.')[:-1])
            bam = pysam.AlignmentFile(bampath)

            te_ins_start = int(self.breakpoints[0])
            te_ins_end   = int(self.breakpoints[1])

            if te_ins_start < 0:
                te_ins_start = 0

            if te_ins_end < 0:
                te_ins_end = 0

            for r in bam.fetch(self.chrom(), te_ins_start, te_ins_end):
                if r.is_secondary or r.is_supplementary:
                    continue

                if r.qname in read_names:
                    continue
            
                cov = 0
                gap = 0
                in_region = False

                for ap in r.get_aligned_pairs(matches_only=False):
                    if not in_region and ap[1] is None:
                        continue

                    if in_region:
                        if ap[1] is None:
                            gap += 1
                            continue
                        else:
                            cov += 1

                    if ap[1] > te_ins_start-wiggle:
                        in_region = True
                        
                    if ap[1] > te_ins_end+wiggle:
                        in_region = False

                if gap < min_te_len and float(cov) > ((te_ins_end-te_ins_start)+(wiggle*2))*cov_pct:
                    self.empty_reads[bamname].append(get_phase(r))

    def empty_reads_count(self):
        if len(self.empty_reads) == 0:
            return 'NA'

        return ','.join(['%s|%d' % (sample, len(self.empty_reads[sample])) for sample in self.empty_reads])

    def trim_reads(self, flanksize=200):
        for read in self.reads:
            read.trim(flanksize=flanksize)

    def adjust_cons(self, left_corr, right_corr):
        left_orig, right_orig = self.sorted_unmapped_segments()[0]

        cons = list(self.cons)

        try:
            if left_corr < 0:
                for b in range(left_orig, left_orig+abs(left_corr)):
                    cons[b] = cons[b].upper()

            if left_corr > 0:
                for b in range(left_orig-left_corr, left_orig):
                    cons[b] = cons[b].lower()

            if right_corr < 0:
                for b in range(right_orig, right_orig+abs(right_corr)):
                    cons[b] = cons[b].lower()

            if right_corr > 0:
                for b in range(right_orig-right_corr, right_orig):
                    cons[b] = cons[b].upper()

            if cons != self.cons:
                self.cons = ''.join(cons)
                return True

        except IndexError as e:
            logger.warning('Index Error on consensus adjustment: %s %s:%s' % (self.uuid, self.chrom(), '-'.join(map(str, self.interval()))))
            return False

        return False

    def extend_tsd(self):
        if self.tsd_seq == 'NA':
            return 0,0

        tsd_len = len(self.tsd_seq)

        left_bp, right_bp = self.sorted_unmapped_segments()[0]

        tsd_left = self.cons[left_bp-tsd_len:left_bp]
        tsd_right = self.cons[right_bp:right_bp+tsd_len]

        cons = list(self.cons)

        # extend left
        l = left_bp-tsd_len-1
        r = right_bp-1

        l_ext = 0
        while l > 0 and cons[l].upper() == cons[r].upper():
            cons[l] = cons[l].upper()
            cons[r] = cons[r].upper()
            self.tsd_seq = cons[l].upper() + self.tsd_seq
            l -= 1
            r -= 1
            l_ext += 1

        # extend right
        l = left_bp
        r = right_bp+tsd_len

        r_ext = 0
        while r < len(cons) and cons[l].upper() == cons[r].upper():
            cons[l] = cons[l].upper()
            cons[r] = cons[r].upper()
            self.tsd_seq = self.tsd_seq + cons[l].upper()
            l += 1
            r += 1
            r_ext += 1

        self.cons = ''.join(cons)

        return l_ext, r_ext


    def sorted_unmapped_segments(self):
        if self.cons_align_map is None:
            return [[0,0]]

        segs = []

        current_seg_start = 0
        current_seg_end = 0

        in_seg = False

        for i, b in enumerate(list(self.cons)):
            if b.islower():
                if not in_seg:
                    in_seg = True
                    current_seg_start = i

            if b.isupper():
                if in_seg:
                    in_seg = False
                    current_seg_end = i

                    segs.append([current_seg_start, current_seg_end])

        if list(self.cons)[-1].islower():
            current_seg_end = len(self.cons)

            segs.append([current_seg_start, current_seg_end])

        if len(segs) == 0:
            return [[0,0]]

        return sorted(segs, key=lambda x: x[1]-x[0], reverse=True)


    def join_unmapped_segments(self, exp_flank_len, telib_seq, minlen=100, minfrac=0.7):
        segs = self.sorted_unmapped_segments()
        assert len(segs) > 1

        if segs[1][1] - segs[1][0] < minlen:
            return None

        # close enough to the expected flank length?
        if segs[1][0] > exp_flank_len*1.5 and segs[1][1] < len(self.cons)-(exp_flank_len*1.5):
            return None

        seg_seq = self.cons[segs[1][0]:segs[1][1]]

        subalign = te_align(telib_seq, seg_seq, te_fa_is_seq=True)

        if subalign is None:
            return None

        if (subalign.max_qry_coord() - subalign.min_qry_coord()) / len(seg_seq) < minfrac:
            return None
        
        cons = list(self.cons)

        for p in range(min(segs[0][0], segs[1][0]), max(segs[0][1], segs[1][1])):
            cons[p] = cons[p].lower()

        self.cons = ''.join(cons)

        return self.cons


    def mask_cons(self, trim=True):
        if self.cons_align_map is None:
            return self.cons

        assert len(self.cons_align_map) == len(self.cons)

        masked_cons = []

        for i in range(len(self.cons_align_map)):
            if self.cons_align_map[i] == 0:
                masked_cons.append(self.cons[i].lower())

            if self.cons_align_map[i] == 1:
                masked_cons.append(self.cons[i].upper())

        if trim:
            # trim forward
            trimmed_cons = []
            trimming = True

            for b in masked_cons:
                if b.isupper():
                    trimming = False

                if not trimming:
                    trimmed_cons.append(b)

            masked_cons = trimmed_cons

            # trim reverse
            trimmed_cons = []
            trimming = True

            for b in masked_cons[::-1]:
                if b.isupper():
                    trimming = False

                if not trimming:
                    trimmed_cons.append(b)

            masked_cons = trimmed_cons[::-1]

        self.cons = ''.join(masked_cons)

        return self.cons


    def te_overlap_zscores(self):
        z = {}
        for read in self.reads:
            if read.te_overlap_frac() > 0.0:
                z[read.r_name] = read.te_overlap_frac()

        o_mean = np.mean(list(z.values()))
        o_std   = np.std(list(z.values()))

        for name, s in z.items():
            if o_std == 0:
                z[name] = 0
            else:
                z[name] = abs(s-o_mean)/o_std

        return z


    def mapq_zscores(self):
        z = {}
        for read in self.reads:
            z[read.r_name] = read.r_mapq

        o_mean = np.mean(list(z.values()))
        o_std   = np.std(list(z.values()))

        for name, s in z.items():
            if o_std == 0:
                z[name] = 0
            else:
                z[name] = abs(s-o_mean)/o_std

        return z


    def prelim_filter(self, minreads=4, embed_minreads=1):
        passed = True

        # minimum cluster size
        if len(self.reads) < int(minreads):
            passed = False

        # require at least one fully embedded insertion
        if len([r for r in self.reads if r.is_ins]) < int(embed_minreads):
            passed = False

        return passed

    def bampath(self, sample):
        for read in self.reads:
            if read.bamname == sample:
                return read.bampath

        return None

    def __len__(self):
        return len(self.reads)

    def phase(self):
        p = dd(list)

        for read in self.reads:
            if not read.useable:
                continue

            p[read.bamname].append(read.phase)

        out = []

        for sample in p:
            out.append(sample + '|' + '|'.join(['%s:%d' % (phase, count) for phase, count in Counter(p[sample]).items()]))

        if out:
            return ';'.join(out)

        else:
            return 'NA'
        
    def empty_phase(self):
        out = []
        for sample in self.empty_reads:
            out.append(sample + '|' + '|'.join(['%s:%d' % (phase, count) for phase, count in Counter(self.empty_reads[sample]).items()]))

        if out:
            return ';'.join(out)
        
        else:
            return 'NA'


class ExonerateAlignment:
    def __init__(self, alignment):
        c = alignment.strip().split('\t')
        self.original  = c
        self.subfamily = c[1]
        self.score     = int(c[2])
        self.qry_start = int(c[3])
        self.qry_end   = int(c[4])
        self.tgt_start = int(c[5])
        self.tgt_end   = int(c[6])
        self.match     = float(c[7])
        self.orient    = c[9]

        if self.orient == '-':
            self.tgt_start, self.tgt_end = self.tgt_end, self.tgt_start

    def __lt__(self, other):
        return self.qry_start < other.qry_start


class AlignmentGroup:
    def __init__(self, alignment):
        self.alignments = []
        self.add_alignment(alignment)

    def add_alignment(self, alignment):
        if len(self.alignments) == 0:
            self.alignments.append(alignment)

        else:
            non_redundant = True

            for a in self.alignments:
                if alignment.qry_start > a.qry_start and alignment.qry_end < a.qry_end:
                    non_redundant = False

            if non_redundant:    
                self.alignments.append(alignment)

    def subfamily(self):
        return self.alignments[0].subfamily

    def best_score(self):
        return max([a.score for a in self.alignments])

    def best_match(self):
        return max([a.match for a in self.alignments])

    def avg_match(self):
        m = 0.0
        l = 0

        for a in self.alignments:
            l += abs(a.tgt_end-a.tgt_start)
            m += abs(a.tgt_end-a.tgt_start)*a.match
        
        if l == 0:
            return 0.0

        return m/l

    def min_qry_coord(self):
        return min([a.qry_start for a in self.alignments])

    def max_qry_coord(self):
        return max([a.qry_end for a in self.alignments])

    def min_tgt_coord(self):
        return min([a.tgt_start for a in self.alignments])

    def max_tgt_coord(self):
        return max([a.tgt_end for a in self.alignments])

    def min_qry_orient(self):
        ''' return orientation of leftmost alignment '''
        return sorted(self.alignments)[0].orient

    def max_qry_orient(self):
        ''' return orientation of rightmost alignment '''
        return sorted(self.alignments)[-1].orient

    def __str__(self):
        return '\n'.join(['\t'.join(a.original) for a in self.alignments])



def rc(dna):
    ''' reverse complement '''
    complements = str.maketrans('acgtrymkbdhvACGTRYMKBDHV', 'tgcayrkmvhdbTGCAYRKMVHDB')
    return dna.translate(complements)[::-1]


def has_homopol(dna, n):
    ''' true if homopolymer in (A,T,G,C) of length >= n in dna seq '''
    dna = dna.upper()
    for b in ('A','T','G','C'):
        if b*int(n) in dna:
            return True

    return False


def get_phase(read):
    phase = 'unphased'

    HP = None
    PS = None

    for tag in read.get_tags():
        if tag[0] == 'HP':
            HP = tag[1]
        if tag[0] == 'PS':
            PS = tag[1]

    if None not in (HP, PS):
        phase = str(PS) + ':' + str(HP)
    
    if PS is None and HP is not None:
        phase=str(PS) + ':NA'
    
    return phase


def mafft(in_fn):
    ''' use MAFFT to create MSA '''

    out_fn = '.'.join(in_fn.split('.')[:-1]) + '.msa.fa'

    args = ['mafft', '--randomseed', '1', in_fn]

    FNULL = open(os.devnull, 'w')

    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=FNULL)

    with open(out_fn, 'w') as out_fa:
        for line in p.stdout:
            line = line.decode()
            out_fa.write(line)

    return out_fn


def te_align(te_fa, refseq, minmatch=80.0, te_fa_is_seq=False, max_segs=3):
    if has_homopol(refseq, 200):
        return None

    rnd = str(uuid4())
    tgt_fa = 'tmp.' + rnd + '.tgt.fa'

    with open(tgt_fa, 'w') as tgt:
        tgt.write('>ref\n%s\n' % refseq)

    if te_fa_is_seq:
        te_seq = te_fa
        te_fa = 'tmp.' + rnd + '.te.fa'
        with open(te_fa, 'w') as te:
            te.write('>te\n%s\n' % te_seq)

    cmd = ['exonerate', '--bestn', str(max_segs), '--model', 'affine:local', '--showalignment','0', '-g', '0', '--ryo', 'TE\t%qi\t%s\t%qab\t%qae\t%tab\t%tae\t%pi\t%qS\t%tS\n', te_fa, tgt_fa]
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    align_groups = {}

    for pline in p.stdout.readlines():
        pline = pline.decode()
        if pline.startswith('TE'):
            a = ExonerateAlignment(pline.strip())
            if a.match >= minmatch:
                if a.subfamily in align_groups:
                    align_groups[a.subfamily].add_alignment(a)

                else:
                    align_groups[a.subfamily] = AlignmentGroup(a)

    
    os.remove(tgt_fa)

    if te_fa_is_seq:
        os.remove(te_fa)

    best_group = None

    for subfam in align_groups:
        if best_group is None:
            best_group = align_groups[subfam]

        elif align_groups[subfam].best_score() > best_group.best_score():
            best_group = align_groups[subfam]

    return best_group


def load_falib(infa):
    seqdict = {}

    with open(infa, 'r') as fa:
        seqid = ''
        seq   = ''
        for line in fa:
            if line.startswith('>'):
                if seq != '':
                    seq = seq.upper()
                    checkseq = []

                    for b in seq:
                        if b in ['A','C','G','T','N']:
                            checkseq.append(b)
                        else:
                            logger.debug('base in %s, id %s, not in (A,C,T,G,N): %s. Changed to "N"' % (infa, seqid, b))
                            checkseq.append('N')

                    seq = ''.join(checkseq)
                        
                    seqdict[seqid] = seq
                seqid = line.lstrip('>').strip().split()[0]

                if ':' not in seqid:
                    logger.error("TE reference sequence .fasta headers must be in the format superfamily:subfamily e.g. >L1:L1Ta")
                    return None

                seq = ''
            else:
                assert seqid != ''
                seq = seq + line.strip()

    if seqid not in seqdict and seq != '':
        seqdict[seqid] = seq

    return seqdict


def minimap2_pileup(cluster, dump_fq, outbase, min_alt_depth=3, min_alt_frac=.5, min_total_depth_frac=0.05):
    out_name = cluster.uuid
    ins_ref = cluster.cons

    tmp_ref = str(uuid4()) + '.fa'
    with open(tmp_ref, 'w') as out:
        out.write('>ins_ref\n%s\n' % ins_ref)

    filter_start, filter_end = cluster.sorted_unmapped_segments()[0]

    FNULL = open(os.devnull, 'w')

    mm2_cmd  = ['minimap2', '-x', 'map-ont', '-a', tmp_ref, dump_fq]
    view_cmd = ['samtools', 'view', '-b', '-']
    sort_cmd = ['samtools', 'sort', '-']
    pile_cmd = ['samtools', 'mpileup', '-B', '-Q', '1', '-f', tmp_ref, '-']
    aln  = subprocess.Popen(mm2_cmd, stdout=subprocess.PIPE, stderr=FNULL)
    view = subprocess.Popen(view_cmd, stdin=aln.stdout, stdout=subprocess.PIPE, stderr=FNULL)
    sort = subprocess.Popen(sort_cmd, stdin=view.stdout, stdout=subprocess.PIPE, stderr=FNULL)
    pile = subprocess.Popen(pile_cmd, stdin=sort.stdout, stdout=subprocess.PIPE, stderr=FNULL)

    pileup = {}
    depth = {}

    for line in pile.stdout:
        c = line.decode().split()
        pileup[int(c[1])] = parse_pileup(c[4])
        depth[int(c[1])] = int(c[3])

    altered_ref = alter_ref(ins_ref, pileup, depth, cluster.te_align_count(), min_alt_depth=min_alt_depth, min_alt_frac=min_alt_frac, min_total_depth_frac=min_total_depth_frac)

    os.remove(tmp_ref)

    if os.path.exists(tmp_ref+'.fai'):
        os.remove(tmp_ref+'.fai')

    return altered_ref


def index_fai(fasta):
    fai_out = fasta + '.fai'

    if os.path.exists(fai_out):
        logger.debug(f'index found for {fasta}: {fai_out}')
        return fai_out
    else:
        logger.debug(f'building fai index for {fasta}: {fai_out}')

    cmd = ['samtools', 'faidx', fasta]
    
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    for line in p.stdout:
        line = line.decode()

    assert os.path.exists(fai_out), f'index failure: {fai_out}'

    return fai_out


def minimap2_bam(ins_ref, dump_fq, outbam, mm, ml, hp, ps):

    FNULL = open(os.devnull, 'w')

    mm2_cmd  = ['minimap2', '-x', 'map-ont', '-a', ins_ref, dump_fq]
    view_cmd = ['samtools', 'view', '-u', '-']
    sort_cmd = ['samtools', 'sort', '-']
    aln  = subprocess.Popen(mm2_cmd, stdout=subprocess.PIPE, stderr=FNULL)
    view = subprocess.Popen(view_cmd, stdin=aln.stdout, stdout=subprocess.PIPE, stderr=FNULL)
    sort = subprocess.Popen(sort_cmd, stdin=view.stdout, stdout=subprocess.PIPE, stderr=FNULL)

    save = pysam.set_verbosity(0)
    bamstream = pysam.AlignmentFile(sort.stdout, 'rb')
    pysam.set_verbosity(save)

    out = pysam.AlignmentFile(outbam, 'wb', header=bamstream.header)

    for read in bamstream:
        if read.qname in mm:
            assert read.qname in ml

            read.set_tag('MM', mm[read.qname])
            read.set_tag('ML', ml[read.qname])

        if read.qname in hp:
            assert read.qname in ps

            read.set_tag('PS', ps[read.qname])
            read.set_tag('HP', hp[read.qname])

        out.write(read)

    out.close()

    subprocess.call(['samtools', 'index', outbam])

    return outbam


def minimap2_breakpoint(ref, cons, side):
    assert side in ('L', 'R')

    tmp_ref = str(uuid4()) + '.fa'
    with open(tmp_ref, 'w') as out:
        out.write('>ref\n%s\n' % ref.upper())

    tmp_cons = str(uuid4()) + '.fa'
    with open(tmp_cons, 'w') as out:
        out.write('>cons\n%s\n' % cons.upper())

    FNULL = open(os.devnull, 'w')

    mm2_cmd  = ['minimap2', '-x', 'map-ont', '-Y', '-a', tmp_ref, tmp_cons]
    view_cmd = ['samtools', 'view', '-b', '-']

    aln  = subprocess.Popen(mm2_cmd, stdout=subprocess.PIPE, stderr=FNULL)
    view = subprocess.Popen(view_cmd, stdin=aln.stdout, stdout=subprocess.PIPE, stderr=FNULL)

    save = pysam.set_verbosity(0)
    bamstream = pysam.AlignmentFile(view.stdout, 'rb')
    pysam.set_verbosity(save)

    for read in bamstream:
        if not read.is_unmapped and not read.is_secondary and not read.is_supplementary:
            if side == 'L':
                os.remove(tmp_ref)
                os.remove(tmp_cons)
                return read.get_aligned_pairs(matches_only=True)[-1]

            if side == 'R':
                os.remove(tmp_ref)
                os.remove(tmp_cons)
                return read.get_aligned_pairs(matches_only=True)[0]

    os.remove(tmp_ref)
    os.remove(tmp_cons)

    return None


def minimap2_extend(ref, cons):

    # mask the repetitive part of the consensus

    cons_masked = []

    for b in cons:
        if b.islower():
            b = 'N'

        cons_masked.append(b)

    cons_masked = ''.join(cons_masked)


    tmp_ref = str(uuid4()) + '.fa'
    with open(tmp_ref, 'w') as out:
        out.write('>ref\n%s\n' % ref.upper())

    tmp_cons = str(uuid4()) + '.fa'
    with open(tmp_cons, 'w') as out:
        out.write('>cons\n%s\n' % cons_masked)

    FNULL = open(os.devnull, 'w')

    mm2_cmd  = ['minimap2', '-x', 'map-ont', '-Y', '-a', tmp_ref, tmp_cons]
    view_cmd = ['samtools', 'view', '-b', '-']
    aln  = subprocess.Popen(mm2_cmd, stdout=subprocess.PIPE, stderr=FNULL)
    view = subprocess.Popen(view_cmd, stdin=aln.stdout, stdout=subprocess.PIPE, stderr=FNULL)

    save = pysam.set_verbosity(0)
    bamstream = pysam.AlignmentFile(view.stdout, 'rb')
    pysam.set_verbosity(save)

    map_ref_start  = None
    map_ref_end    = None
    map_cons_start = None
    map_cons_end   = None

    for read in bamstream:
        if not read.is_unmapped:
            for q, r in read.get_aligned_pairs(matches_only=True):
                if map_ref_start is None:
                    map_ref_start = r
                    map_cons_start = q

                elif map_ref_start > r:
                    map_ref_start = r
                    map_cons_start = q

                if map_ref_end is None:
                    map_ref_end = r
                    map_cons_end = q

                elif map_ref_end < r:
                    map_ref_end = r
                    map_cons_end = q

    os.remove(tmp_ref)
    os.remove(tmp_cons)

    if None in (map_ref_start, map_ref_end, map_cons_start, map_cons_end):
        return cons

    extended_seq = ref[:map_ref_start] + cons[map_cons_start:map_cons_end] + ref[map_ref_end:]

    if len(extended_seq) <= len(cons):
        return cons

    return extended_seq


def minimap2_profile(ref, cons, max_de = 0.12):
    tmp_ref = str(uuid4()) + '.fa'
    with open(tmp_ref, 'w') as out:
        out.write('>ref\n%s\n' % ref.upper())

    tmp_cons = str(uuid4()) + '.fa'
    with open(tmp_cons, 'w') as out:
        out.write('>cons\n%s\n' % cons)

    FNULL = open(os.devnull, 'w')

    mm2_cmd  = ['minimap2', '-x', 'map-ont', '-Y', '-a', tmp_ref, tmp_cons]
    view_cmd = ['samtools', 'view', '-b', '-']
    aln  = subprocess.Popen(mm2_cmd, stdout=subprocess.PIPE, stderr=FNULL)
    view = subprocess.Popen(view_cmd, stdin=aln.stdout, stdout=subprocess.PIPE, stderr=FNULL)

    save = pysam.set_verbosity(0)  # workaround for htslib diagnostic message
    bamstream = pysam.AlignmentFile(view.stdout, 'rb')
    pysam.set_verbosity(save)

    profiles = []

    for read in bamstream:
        if not read.is_unmapped:
            if read.has_tag('de'): # gap-compressed per-base divergence
                if read.get_tag('de') > max_de:
                    continue

            profile = np.zeros(len(cons))-1

            b = 0
            for cig in read.cigartuples:
                if cig[0] in (0,1,4): # matches, insertions, and soft clips add bases to reads
                    profile[b:b+cig[1]] = cig[0]
                    b += cig[1]

            profiles.append(profile)

    if len(profiles) == 0:
        os.remove(tmp_ref)
        os.remove(tmp_cons)
        return None

    merged_profile = []

    for i in range(len(profiles[0])):
        matched = 0
        for p in profiles:
            assert int(p[i]) in (0,1,4)

            if p[i] == 0: # match
                matched = 1

        merged_profile.append(matched)

    os.remove(tmp_ref)
    os.remove(tmp_cons)

    return merged_profile


def parse_pileup(pile):
    pile = pile.upper()
    parse = []

    i = 0
    while i < len(pile):
        if pile[i] in (',','.','A','T','C','G','*'):
            parse.append(pile[i])
            i += 1

        elif pile[i] in ('+','-'):
            op = [pile[i]]
            i += 1

            num_bases = [pile[i]]
            i += 1

            while pile[i] in map(str, list(range(10))):
                num_bases.append(pile[i])
                i += 1

            num_bases = int(''.join(num_bases))

            for _ in range(num_bases):
                op.append(pile[i])
                i += 1

            parse.append(''.join(op))

        elif pile[i] == '^':
            i += 2 # char after "^" is mapq

        else:
            i += 1

    return parse


def mixmodel(pos, reg_covar=1e-4, max_iter=1000, max_pos=2):
    pos = np.asarray(pos).reshape(-1,1)

    N = np.arange(1,max_pos+1)
    models = [None for i in range(len(N))]

    for i in range(len(N)):
        models[i] = GaussianMixture(N[i], reg_covar=reg_covar, max_iter=max_iter, n_init=max_pos, random_state=1).fit(pos)

    AIC = [m.aic(pos) for m in models]

    return models[np.argmin(AIC)] # best-fit mixture


def alter_ref(ins_ref, pileup, depth, te_align_count, min_alt_depth=3, min_alt_frac=.5, min_total_depth_frac=0.1):#, filter_start, filter_end):
    altered = []

    # mpileup coords are 1-based
    for i, b in enumerate(ins_ref, 1):

        if i in pileup:
            if depth[i] < te_align_count*min_total_depth_frac:
                continue

            snv_p = [p for p in pileup[i] if p[0] not in ('+', '-', '.', ',')]
            ins_p = [p for p in pileup[i] if p[0] == '+']

            alt_count = 0
            alt_base = ''

            if snv_p:
                alt_base, alt_count = Counter(snv_p).most_common()[0]

            ins_count = 0
            ins_seq = ''

            if ins_p:
                ins_seq, ins_count = Counter(ins_p).most_common()[0]

            if alt_count > depth[i]*min_alt_frac and alt_count >= min_alt_depth:
                b = alt_base

            if ins_count > depth[i]*min_alt_frac and ins_count >= min_alt_depth:
                b += ins_seq.lstrip('+')

            if b != '*': # deletion
                altered.append(b)

    return ''.join(altered)


def process_cluster(cluster, inslib, outbase, args, noref_mode):
    minreads = int(args.minreads)
    embed_minreads = int(args.embed_minreads)

    tracking = False
    if args.trackread and args.trackread in [read.r_name for read in cluster.reads]:
        logger.debug('processing cluster %s containing tracked read %s' % (cluster.uuid, args.trackread))
        tracking = True

    max_cluster_size = None
    if args.max_cluster_size:
        max_cluster_size = int(args.max_cluster_size)

    te_lib = args.elts
    ref = pysam.Fastafile(args.ref)

    if cluster.prelim_filter(minreads=minreads, embed_minreads=embed_minreads):
        logger.debug('processing cluster %s, length: %d' % (cluster.uuid, len(cluster)))

        if len(cluster) > 500 and (max_cluster_size is None):
            logger.warning('found a cluster with length > 500, consider --max_cluster_size if runtime is too long')

        if max_cluster_size:
            if len(cluster) > max_cluster_size:
                cluster = cluster.downsample(max_cluster_size)
                logger.debug('downsampled cluster %s to %d reads' % (cluster.uuid, max_cluster_size))

                if tracking:
                    if args.trackread not in [read.r_name for read in cluster.reads]:
                        logger.debug('lost tracked read %s due to downsampling (--max_cluster_size)')
                        tracking = False

        cluster.trim_reads(int(args.flanksize))

        for insread in cluster.reads:
            te = te_align(te_lib, insread.r_seq_trimmed)
            if te:
                insread.te_info(te)

        if cluster.te_align_count() >= minreads or noref_mode:
            major_family = cluster.te_major_family()
            major_subfam = cluster.te_major_subfam()
            family_frac  = cluster.te_fam_frac(major_family)

            use_reads = []

            if noref_mode:
                for r_name, z in cluster.mapq_zscores().items():
                    if abs(z) < 2:
                        use_reads.append(r_name)

            else:
                for r_name, z in cluster.te_overlap_zscores().items():
                    if abs(z) < 2:
                        use_reads.append(r_name)

            for read in cluster.reads:
                if (read.r_name in use_reads and read.te) or noref_mode:
                    read.useable = True

            if tracking:
                for read in cluster.reads:
                    if read.r_name == args.trackread:
                        if not read.useable:
                            logger.debug('tracked read %s marked unusable' % args.trackread)
                            tracking = False

            if noref_mode:
                major_family = 'NR'
                major_subfam = 'NR'
                family_frac = 1.0

            if family_frac < 0.5 or None in (major_family, major_subfam):
                if tracking:
                    logger.debug('dropped cluster containing tracked read %s due to subfamily mismatch' % args.trackread)
                return None

            te_type = major_family+':'+major_subfam

            # set initial breakpoints from GMM
            r_pos_list = sorted([read.r_pos for read in cluster.reads if read.useable])

            if len(r_pos_list) == 0:
                if tracking:
                    logger.debug('dropped cluster containing tracked read %s due to lack of breakpoints' % args.trackread)
                return None

            elif len(r_pos_list) == 1:
                cluster.breakpoints = [r_pos_list[0], r_pos_list[0]]

            elif len(r_pos_list) == 2:
                cluster.breakpoints = r_pos_list

            elif len(set(r_pos_list)) == 1:
                cluster.breakpoints = [r_pos_list[0], r_pos_list[0]]

            else:
                model = mixmodel(r_pos_list)

                means = list(model.means_.reshape(1,-1)[0])
                if len(means) == 1:
                    means.append(means[0])

                cluster.breakpoints = sorted(map(round, means))

            dump_fa = cluster.dump_fasta(te_fam=major_family)
            dump_fq = cluster.dump_fastq(te_fam=major_family)
            msa_fa = mafft(dump_fa)
            msa = MSA(msa_fa)

            if len(cluster) == 1 and int(args.minreads) == 1:
                cluster.cons = cluster.reads[0].r_seq_trimmed

            else:
                cluster.cons = msa.consensus(gapped=False)

            cluster.cons = minimap2_pileup(cluster, dump_fq, outbase, min_alt_depth=int(args.min_alt_depth), min_alt_frac=float(args.min_alt_frac), min_total_depth_frac=float(args.min_total_depth_frac))

            if noref_mode or te_type in inslib:
                samples = list(set([read.bamname for read in cluster.reads if read.useable]))

                rr_start = int(cluster.breakpoints[0]) - int(args.flanksize)
                rr_end   = int(cluster.breakpoints[1]) + int(args.flanksize)

                if rr_start < 0:
                    rr_start = 0

                if rr_end < 0:
                    rr_end = 0

                ref_region = ref.fetch(cluster.chrom(), rr_start, rr_end)
                
                cluster.cons_align_map = minimap2_profile(ref_region, cluster.cons)
                cluster.mask_cons()

                cons_seg_coords = cluster.sorted_unmapped_segments()[0]
                cons_seg_seq = cluster.cons[cons_seg_coords[0]:cons_seg_coords[1]]

                # this alignment can overrule previous subfamily classification
                cluster.cons_te_align = te_align(te_lib, cons_seg_seq)

                if cluster.cons_te_align is None:
                    cluster.reset_major_family, cluster.reset_major_subfam = 'NA', 'NA'

                else:
                    subfam = cluster.cons_te_align.subfamily()
                    if ':' in subfam:
                        cluster.reset_major_family, cluster.reset_major_subfam = subfam.split(':')
                    else:
                        cluster.reset_major_family = subfam
                        cluster.reset_major_subfam = subfam

                # handle case where te alignment is split across > 1 unaligned segment
                if cluster.cons_te_align is not None and len(cluster.sorted_unmapped_segments()) > 1:
                    logger.debug('joining multiple segments for cluster %s' % cluster.uuid)

                    joined_cons = cluster.join_unmapped_segments(int(args.flanksize), inslib[cluster.cons_te_align.subfamily()])

                    if joined_cons is not None:
                        # repeat te alignment, subfam reassignment if unmapped segments have been joined
                        cons_seg_coords = cluster.sorted_unmapped_segments()[0]
                        cons_seg_seq = cluster.cons[cons_seg_coords[0]:cons_seg_coords[1]]

                        cluster.cons_te_align = te_align(te_lib, cons_seg_seq)

                        if cluster.cons_te_align is None:
                            cluster.reset_major_family, cluster.reset_major_subfam = 'NA', 'NA'

                        else:
                            subfam = cluster.cons_te_align.subfamily()
                            if ':' in subfam:
                                cluster.reset_major_family, cluster.reset_major_subfam = subfam.split(':')
                            else:
                                cluster.reset_major_family = subfam
                                cluster.reset_major_subfam = subfam

                # clean up breakpoints
                bp_left, bp_right = cluster.sorted_unmapped_segments()[0]

                bp_left_seq = cluster.cons[bp_left-150:bp_left+50]
                bp_right_seq = cluster.cons[bp_right-50:bp_right+150]

                break_left = minimap2_breakpoint(ref_region, bp_left_seq, 'L')
                break_right = minimap2_breakpoint(ref_region, bp_right_seq, 'R')

                if None not in (break_left, break_right):
                    # fine-tune breakpoints in consensus seq
                    left_corr = 150-break_left[0]
                    right_corr = 50-break_right[0]

                    adjusted = cluster.adjust_cons(left_corr, right_corr)

                    if adjusted:
                        logger.debug('breakpoint adjustment for %s' % cluster.uuid)
                        # repeat te alignment, subfam reassignment if breakpoints have been tweaked
                        cons_seg_coords = cluster.sorted_unmapped_segments()[0]
                        cons_seg_seq = cluster.cons[cons_seg_coords[0]:cons_seg_coords[1]]

                        cluster.cons_te_align = te_align(te_lib, cons_seg_seq)

                        if cluster.cons_te_align is None:
                            cluster.reset_major_family, cluster.reset_major_subfam = 'NA', 'NA'

                        else:
                            subfam = cluster.cons_te_align.subfamily()
                            if ':' in subfam:
                                cluster.reset_major_family, cluster.reset_major_subfam = subfam.split(':')
                            else:
                                cluster.reset_major_family = subfam
                                cluster.reset_major_subfam = subfam
                        
                    # index from start of reference segment
                    cluster.breakpoints = sorted([(int(cluster.breakpoints[0]) - int(args.flanksize)) + break_left[1], 
                                                  (int(cluster.breakpoints[0]) - int(args.flanksize)) + break_right[1]])

                    cluster.breakpoints_remappable = True

                    #TSD
                    if break_right[1] < break_left[1]:
                        cluster.tsd_seq = ref_region[break_right[1]:break_left[1]]

                    l_ext, r_ext = cluster.extend_tsd()

                    if l_ext + r_ext > 0:
                        # repeat te alignment, subfam reassignment if breakpoints have been tweaked
                        cons_seg_coords = cluster.sorted_unmapped_segments()[0]
                        cons_seg_seq = cluster.cons[cons_seg_coords[0]:cons_seg_coords[1]]

                        cluster.cons_te_align = te_align(te_lib, cons_seg_seq)

                        if cluster.cons_te_align is None:
                            cluster.reset_major_family, cluster.reset_major_subfam = 'NA', 'NA'

                        else:
                            subfam = cluster.cons_te_align.subfamily()
                            if ':' in subfam:
                                cluster.reset_major_family, cluster.reset_major_subfam = subfam.split(':')
                            else:
                                cluster.reset_major_family = subfam
                                cluster.reset_major_subfam = subfam

                if args.detail_output:
                    out_cons = cluster.cons

                    out_start = int(cluster.breakpoints[0]) - int(args.flanksize)
                    out_end = int(cluster.breakpoints[1]) + int(args.flanksize)

                    if int(args.extend_consensus) > 0:
                        ext_bases = int(args.extend_consensus)

                        out_start -= ext_bases
                        out_end += ext_bases

                        if out_start < 0:
                            out_start = 0
                        
                        if out_end < 0:
                            out_end = 0

                        ext_ref = ref.fetch(cluster.chrom(), out_start, out_end)
                        out_cons = minimap2_extend(ext_ref, cluster.cons)

                    cons_out_fa = f'{outbase}/{cluster.uuid}.cons.ref.fa'

                    with open(cons_out_fa, 'w') as out:
                        out.write('>%s\n%s\n' % (cluster.uuid.split('-')[0], out_cons))

                    outbams = []

                    for sample in samples:
                        # add surrounding reads to fq
                        bampath = cluster.bampath(sample)

                        per_bam_fq = '%s/%s.%s.cluster.fq' % (outbase, sample, cluster.uuid.split('-')[0])
                        
                        with open(per_bam_fq, 'w') as out_fq:
                            bam = pysam.AlignmentFile(bampath)

                            if out_start < 0:
                                out_start = 0
                            
                            if out_end < 0:
                                out_end = 0

                            # track mod calls if present
                            mm_dict = {}
                            ml_dict = {}

                            # track phasing if present
                            hp_dict = {}
                            ps_dict = {}

                            for read in bam.fetch(cluster.chrom(), out_start, out_end):
                                if not read.is_secondary and not read.is_supplementary:
                                    seq = read.seq
                                    qual = read.qual

                                    if None in (seq, qual):
                                        logger.warning('skipped a read without seq/qual: %s' % read.qname)
                                        continue

                                    if read.has_tag('Mm'):
                                        mm_dict[read.qname] = read.get_tag('Mm')
                                    if read.has_tag('MM'):
                                        mm_dict[read.qname] = read.get_tag('MM')

                                    if read.has_tag('Ml'):
                                        ml_dict[read.qname] = read.get_tag('Ml')
                                    if read.has_tag('ML'):
                                        ml_dict[read.qname] = read.get_tag('ML')

                                    if read.has_tag('HP'):
                                        hp_dict[read.qname] = read.get_tag('HP')

                                    if read.has_tag('PS'):
                                        ps_dict[read.qname] = read.get_tag('PS')

                                    if read.is_reverse:
                                        seq = rc(seq)
                                        qual = qual[::-1]

                                    out_fq.write('@%s\n%s\n+\n%s\n' % (read.query_name, seq, qual))

                        te_outbam = minimap2_bam(cons_out_fa, per_bam_fq, '%s/%s.%s.te.bam' % (outbase, sample, cluster.uuid), mm_dict, ml_dict, hp_dict, ps_dict)
                        outbams.append(te_outbam)

                        os.remove(per_bam_fq)

                    if args.methylartist:
                        cons_aln = te_align(cons_seg_seq, out_cons, minmatch=95.0, te_fa_is_seq=True)
                        if cons_aln is None:
                            logger.debug(f'warning: cons_aln is None at {cluster.uuid}')
                        
                        else:
                            h_l = cons_aln.min_tgt_coord()
                            h_r = cons_aln.max_tgt_coord()

                            index_fai(cons_out_fa)
                            bamlist = ','.join([os.path.abspath(b) for b in outbams])

                            ma_bed = f'{outbase}/{sample}.{cluster.uuid}.bed'

                            with open(ma_bed, 'w') as bed_out:
                                bed_out.write(f'{cluster.uuid.split("-")[0]}\t{h_l}\t{h_r}\t{cluster.te_major_subfam()}\t{cluster.te_orientation()}\n')

                            ma_fn = f'{outbase}/{sample}.{cluster.uuid}.methylartist.cmd'

                            with open(ma_fn, 'w') as ma_out:
                                ma_loc_cmd = f'methylartist locus -i {cluster.uuid.split("-")[0]}:{0}-{len(out_cons)} -l {h_l}-{h_r} -b {bamlist} -r {os.path.abspath(cons_out_fa)} --motif CG --nomask --bed {os.path.abspath(ma_bed)}\n'
                                ma_seg_cmd = f'methylartist segmeth -b {bamlist} -i {os.path.abspath(ma_bed)} --ref {os.path.abspath(cons_out_fa)} --motif CG\n'
                                ma_out.write(ma_loc_cmd)
                                ma_out.write(ma_seg_cmd)

                cluster.spanning_non_supporting_reads(int(args.wiggle), int(args.min_te_len))

            else:
                if tracking:
                    logger.debug('cluster containing tracked read %s failed prelim filters' % args.trackread)

            os.remove(msa_fa)
            os.remove(dump_fa)
            os.remove(dump_fq)

    return cluster


def build_clusters(args, outbase, chrom):
    min_te_len = int(args.min_te_len)
    max_te_len = int(args.max_te_len)
    wiggle = int(args.wiggle)

    bams = [pysam.AlignmentFile(bam) for bam in args.bams.split(',')]

    ins_forest = dd(Intersecter)
    clusters = dict()

    targets = None
    if args.targets is not None:
        targets = Intersecter()

        with open(args.targets) as target_bed:
            for line in target_bed:
                tf_chr, tf_start, tf_end = line.strip().split()[:3]
                tf_start = int(tf_start)-wiggle
                tf_end   = int(tf_end)+wiggle
                if tf_start == tf_end:
                    tf_end += 1

                if tf_chr == chrom:
                    targets.add_interval(Interval(tf_start, tf_end))

    for bam in bams:
        for read in bam.fetch(contig=chrom):
            tracking = False
            if args.trackread and read.qname == args.trackread:
                tracking = True

            if read.is_secondary or read.is_supplementary:
                if tracking:
                    logger.debug('ignored non-primary alignment for tracked read %s' % args.trackread)
                continue

            if targets is not None:
                target_in_read = False
                for tf_rec in targets.find(read.reference_start, read.reference_end):
                    target_in_read = True

                if not target_in_read:
                    if tracking:
                        logger.debug('tracked read %s was not in --targets' % args.trackread)
                    continue

            b = 0  # query position within read

            for cig in read.cigartuples:
                if cig[1] > min_te_len and cig[0] in (1,4): # insertion or soft clip in read
                    is_ins  = cig[0] == 1
                    is_clip = cig[0] == 4

                    if is_clip and cig[1] > max_te_len:
                        continue

                    clip_end = None
                    if is_clip:
                        clip_end = 'R'


                    q_start = b-1
                    if q_start < 0:
                        q_start = 0
                        clip_end = 'L'

                    q_end = b+cig[1]

                    ap = dict(read.get_aligned_pairs())

                    r_pos = None

                    if q_start in ap:
                        r_pos = ap[q_start]

                    if r_pos is None and q_end in ap:
                        r_pos = ap[q_end]

                    if r_pos is None:
                        logger.debug('skip read warning: %s' % read.qname)
                        continue

                    r_start = None
                    r_end = None

                    if q_start in ap:
                        r_start = ap[q_start]

                    if q_end in ap:
                        r_end = ap[q_end]

                    phase = get_phase(read)

                    ins_read = InsRead(bam.filename.decode(), read.reference_name, q_start, q_end, r_start, r_end, read.qname, read.seq, read.qual, read.mapq, is_ins, is_clip, clip_end, phase)

                    cluster_id = None

                    if read.reference_name in ins_forest:
                        for i, cluster in enumerate(ins_forest[read.reference_name].find(r_pos-1,r_pos+1)):
                            cluster_id = cluster.value
                            clusters[cluster_id].add(ins_read)

                            if i > 0:
                                logger.debug('r_pos overlaps multiple clusters on read: %s' % read.qname)

                    if cluster_id is None:
                        cluster_id = str(uuid4())
                        ins_forest[read.reference_name].add_interval(Interval(r_pos-wiggle, r_pos+wiggle, value=cluster_id))
                        clusters[cluster_id] = InsCluster(cluster_id)
                        clusters[cluster_id].add(ins_read)

                    if tracking:
                        logger.debug('tracked read %s added to cluster %s' % (args.trackread, cluster_id))

                if cig[0] in (0,1,4): # matches, insertions, and soft clips add bases to reads
                    b += cig[1] 

    pickle_fn = '%s/%s.pickle' % (outbase, chrom)

    if len(clusters) > 0:
        logger.info('writing clusters to %s' % pickle_fn)

        with open(pickle_fn, 'wb') as p_out:
            pickle.dump(clusters, p_out)

        return pickle_fn

    return None


def assign_filter(output, c_unmap, args, noref_mode, cluster_forest):
    filters = []

    left_flank_len = c_unmap[0]
    right_flank_len = len(output['Consensus']) - c_unmap[1]

    if left_flank_len*2 < int(args.flanksize) or left_flank_len > int(args.flanksize)*2:
        filters.append('LeftFlankSize')

    if right_flank_len*2 < int(args.flanksize) or right_flank_len > int(args.flanksize)*2:
        filters.append('RightFlankSize')

    if not noref_mode:
        if output['UnmapCover'] == 'NA':
            filters.append('UnmapCoverNA')

        elif output['UnmapCover'] < 0.5:
            filters.append('UnmapCover<0.5')

        if output['Family'] == 'NA':
            filters.append('NoFamily')

        if output['StartTE'] == 'NA':
            filters.append('NoTEAlignment')

    if not output['Remappable']:
        filters.append('NonRemappable')

    if 'NA' not in (output['StartTE'], output['EndTE']):
        if (output['EndTE'] - output['StartTE'])*2 < output['LengthIns']:
            if output['Family'] not in accordion_elts:
                filters.append('TEMapTooLong')

    if len(output['TSD']) > 100:
        filters.append('TSDOver100bp')

    if output['LengthIns'] < int(args.min_te_len):
        filters.append('ShortIns')

    if int(output['MedianMapQ']) == 0:
        filters.append('ZeroMapQ')

    for c in cluster_forest[output['Chrom']].find(int(output['Start']), int(output['End'])):
        if c.value['UUID'] != output['UUID']:
            if int(output['UsedReads']) < c.value['UsedReads']:
                filters.append('Duplicate')

    if len(filters) > 0:
        return ','.join(filters)
    
    return 'PASS'


def output_table(args, output_recs, header, outbase):
    out_fn = outbase+'.table.txt'

    seqdict = {}
    lendict = {}

    for rec in output_recs:
        seqdict[rec['UUID']] = rec['Consensus']
        lendict[rec['UUID']] = rec['LengthIns']

    if args.strict:
        pass_remap = remap_full_check(lendict, seqdict, args.ref)
        flank_mapq = remap_flanks(seqdict, args.ref)

    with open(out_fn, 'w') as table_out:
        table_out.write('%s\n' % '\t'.join(header))

        for rec in output_recs:
            if args.strict:
                if not pass_remap[rec['UUID']]:
                    if rec['Filter'] == 'PASS':
                        rec['Filter'] = 'AltIns'
                    else:
                        rec['Filter'] = rec['Filter'] + ',AltIns'

                if max(flank_mapq[rec['UUID']]) < 10:
                    if rec['Filter'] == 'PASS':
                        rec['Filter'] = 'FlankMapQ'
                    else:
                        rec['Filter'] = rec['Filter'] + ',FlankMapQ'
        
            table_out.write('%s\n' % '\t'.join(map(str, [rec[col] for col in header])))


def strip_seq(seq):
     nt = ['A', 'T', 'C', 'G']
     return ''.join([b for b in list(seq.upper()) if b in nt])


def remap_full_check(lendict, seqdict, ref, flanksize=500):
    success = {}

    for id, seq in seqdict.items():
        seqdict[id] = strip_seq(seq)

    tmp_cons = str(uuid4()) + '.fa'

    with open(tmp_cons, 'w') as out:
        for id, seq in seqdict.items():
            out.write(f'>{id}\n{seq}\n')
            success[id] = True

    FNULL = open(os.devnull, 'w')

    mm2_cmd  = ['minimap2', '-x', 'map-ont', '-Y', '-a', ref, tmp_cons]
    view_cmd = ['samtools', 'view', '-b', '-']

    aln  = subprocess.Popen(mm2_cmd, stdout=subprocess.PIPE, stderr=FNULL)
    view = subprocess.Popen(view_cmd, stdin=aln.stdout, stdout=subprocess.PIPE, stderr=FNULL)

    save = pysam.set_verbosity(0)
    bamstream = pysam.AlignmentFile(view.stdout, 'rb')
    pysam.set_verbosity(save)

    n_rp_list = dd(list)

    for read in bamstream:
        if not read.is_unmapped:
            inslen = int(lendict[read.query_name])
            n_rp = len(read.get_reference_positions())
            exp_len = len(read.query_sequence)-inslen*.5

            if read.cigartuples[0][0]==4 and read.cigartuples[0][1] > flanksize/2:
                continue

            if read.cigartuples[-1][0]==4 and read.cigartuples[-1][1] > flanksize/2:
                continue

            n_rp_list[read.query_name].append(n_rp)

            if n_rp > exp_len:
                success[read.query_name] = False

    for uuid in n_rp_list:
        s0 = sorted(n_rp_list[uuid], reverse=True)[0]
        s1 = 0

        if len(n_rp_list[uuid]) > 1:
            s1 = sorted(n_rp_list[uuid], reverse=True)[1]

        if s1*1.1 > s0:
            success[uuid] = False

    os.remove(tmp_cons)

    return success


def remap_flanks(seqdict, ref, flanksize=500):
    flank_mapq = {}

    for id, seq in seqdict.items():
        seqdict[id] = strip_seq(seq)

    tmp_cons = str(uuid4()) + '.fa'

    with open(tmp_cons, 'w') as out:
        for id, seq in seqdict.items():
            left_seq = seq[:flanksize]
            right_seq = seq[-flanksize:]

            out.write(f'>{id}.0\n{left_seq}\n')
            out.write(f'>{id}.1\n{right_seq}\n')

            flank_mapq[id] = [-1,-1]

    FNULL = open(os.devnull, 'w')

    mm2_cmd  = ['minimap2', '-x', 'map-ont', '-Y', '-a', ref, tmp_cons]
    view_cmd = ['samtools', 'view', '-b', '-']

    aln  = subprocess.Popen(mm2_cmd, stdout=subprocess.PIPE, stderr=FNULL)
    view = subprocess.Popen(view_cmd, stdin=aln.stdout, stdout=subprocess.PIPE, stderr=FNULL)

    save = pysam.set_verbosity(0)
    bamstream = pysam.AlignmentFile(view.stdout, 'rb')
    pysam.set_verbosity(save)

    n_rp_list = dd(list)

    for read in bamstream:
        if not read.is_unmapped:
            uuid = '.'.join(read.query_name.split('.')[:-1])
            flank = int(read.query_name.split('.')[-1])
            assert flank in (0,1)
            mapq = read.mapping_quality

            if flank_mapq[uuid][flank] == -1:
                flank_mapq[uuid][flank] = mapq
            else:
                if mapq < flank_mapq[uuid][flank]:
                    flank_mapq[uuid][flank] = mapq

    os.remove(tmp_cons)

    return flank_mapq


def predict_somatic(rec, max_vaf=0.4, min_mapq=10.0):
    nonref = rec['NonRef']
    num_samples = int(rec['NumSamples'])
    filled_phase = rec['Phasing']
    empty_phase = rec['EmptyPhase']

    if float(rec['MedianMapQ']) < min_mapq:
        return 'UNKNOWN'

    if num_samples > 1:
        return 'GERMLINE'
    
    if nonref != 'NA':
        return 'GERMLINE'

    if 'NA' in (filled_phase, empty_phase):
        return 'UNKNOWN'

    fph = dd(list)
    eph = dd(list)

    fph_info = filled_phase.split("|")[1:]
    eph_info = empty_phase.split("|")[1:]

    shared_ps = [] # phase sets in filled and empty

    for ph in fph_info:
        if ph.startswith('unphased'):
            continue

        ps, hp, rc = ph.split(':')
        if int(rc) >= 1:
            fph[ps].append((hp, int(rc)))
    
    for ph in eph_info:
        if ph.startswith('unphased'):
            continue

        ps, hp, rc = ph.split(':')
        if int(rc) >= 2: # more stringent about both alleles in empty site
            eph[ps].append((hp, int(rc)))

        if ps in fph:
            shared_ps.append(ps)
    
    shared_ps = list(set(shared_ps))

    if len(shared_ps) == 0:
        return 'UNKNOWN'

    for ps in shared_ps:
        if not (len(fph[ps]) == 1 and len(eph[ps]) > 1):
            return 'GERMLINE'

    for ps in shared_ps:
        f_count = sum([ph[1] for ph in fph[ps]])
        e_count = sum([ph[1] for ph in eph[ps]])

        if f_count/e_count > max_vaf:
            return 'UNKNOWN'

    return 'SOMATIC'


header = [
'UUID',
'Chrom',
'Start',
'End',
'Strand',
'Family',
'Subfamily',
'StartTE',
'EndTE',
'LengthIns',
'Inversion',
'UnmapCover',
'MedianMapQ',
'TEMatch',
'UsedReads',
'SpanReads',
'NumSamples',
'SampleReads',
'EmptyReads',
'EmptyPhase',
'NonRef',
'TSD',
'Consensus',
'Phasing',
'Remappable',
'Filter'
]

detail_header = [
'Cluster',
'BamName',
'ReadName',
'IsSpanRead',
'TEAlign',
'TEOverlap',
'Useable',
'RefPos',
'Phase'
]

accordion_elts = [
'SVA'
]


def main(args):

    if args.debug:
        logger.setLevel(logging.DEBUG)

    if args.methylartist:
        if args.extend_consensus == 0:
            logger.warning(f'NOTE: setting --extend_consensus is recommended in conjunction with --methylartist if you intend to plot the results')

        if not args.detail_output:
            logger.warning(f'NOTE: --methylartist does nothing without --detail_output')

    nonref = None
    if args.nonref:
        nonref = pysam.Tabixfile(args.nonref)

    bams = args.bams.split(',')

    outbase = '_'.join(['.'.join(os.path.basename(bam).split('.')[:-1]) for bam in bams])

    if args.outbase is not None:
        outbase = args.outbase

    logger.info('tldr started with command: %s' % ' '.join(sys.argv))
    logger.info('output basename: %s' % outbase)

    if not os.path.exists(outbase):
        os.mkdir(outbase)

    bam = pysam.AlignmentFile(bams[0])
    chroms = bam.references

    if args.chroms is not None:
        chroms = []
        with open(args.chroms) as _:
            for line in _:
                chroms.append(line.strip())

    noref_mode = False

    if args.elts.upper() == 'NONE':
        logger.info('"None" passed to -e/--elts, running without TE reference')
        noref_mode = True

    inslib = None

    if not noref_mode:
        inslib = load_falib(args.elts)
        if inslib is None:
            logger.error("loading insertion library %s failed." % args.elts)
            sys.exit(1)

    if args.trdcol:
        if noref_mode:
            logger.warning('--trdcol with noref mode: transductions not meaningful without TE reference')

        header.append('Transduction_5p')
        header.append('Transduction_3p')

    if args.somatic:
        header.append('Somatic')
        args.strict = True

    output_recs = []
    targets = None

    if args.targets is not None:
        targets = dd(Intersecter)

        with open(args.targets) as target_bed:
            for line in target_bed:
                tf_chr, tf_start, tf_end = line.strip().split()[:3]
                tf_start = int(tf_start)-int(args.wiggle)
                tf_end   = int(tf_end)+int(args.wiggle)

                if tf_end == tf_start:
                    logger.warning('record in --targets has length 0 (%s) incremeted end +1' % line.strip())
                    tf_end += 1

                targets[tf_chr].add_interval(Interval(tf_start, tf_end))

    if targets is not None:
        chroms = list(targets.keys())
        logger.info('searching chromosomes represented in --targets: %s' % ','.join(chroms))

    pool = mp.Pool(processes=int(args.procs))

    pickles = []

    if args.use_pickles:
        assert os.path.exists(args.use_pickles)

        for pickle_fn in os.listdir(args.use_pickles):
            if pickle_fn.endswith('.pickle'):
                pickles.append(args.use_pickles + '/' + pickle_fn)

        logger.info('--use_pickles, found %d .pickle files in %s' % (len(pickles), args.use_pickles))

    else:  # parallelise cluster building across chromosomes
        reslist = []
        for chrom in chroms:
            res = pool.apply_async(build_clusters, [args, outbase, chrom])
            reslist.append(res)

        for res in reslist:
            pickle_fn = res.get()
            if pickle_fn is not None:
                pickles.append(pickle_fn)

    for pickle_fn in pickles:
        clusters = []
        gc.collect()

        with open(pickle_fn, 'rb') as p:
            p_clusters = pickle.load(p)

            if len(p_clusters) == 0:
                continue

            logger.info('loaded %d clusters from %s' % (len(p_clusters), pickle_fn))

            clusters = p_clusters.values()

            # parallelise cluster processing per cluster
            reslist = []
            for cluster in clusters:
                res = pool.apply_async(process_cluster, [cluster, inslib, outbase, args, noref_mode])
                reslist.append(res)

            logger.info('processing clusters')

            processed_clusters = []
            for res in tqdm(reslist):
                processed_clusters.append(res.get())

            c_count = 0
            c_tot = 1

            cluster_forest = dd(Intersecter)

            logger.debug('build interval trees for duplicate searching...')

            for cluster in processed_clusters:
                if cluster and cluster.cons:
                    cinfo = {}
                    cinfo['UUID'] = cluster.uuid
                    cinfo['UsedReads'] = cluster.te_useable_count()
                    cluster_forest[cluster.chrom()].add_interval(Interval(int(cluster.breakpoints[0]), int(cluster.breakpoints[1]), value=cinfo))

            for cluster in tqdm(processed_clusters):
                if cluster and cluster.cons:
                    tracking = False

                    if args.trackread and args.trackread in [read.r_name for read in cluster.reads]:
                        logger.debug('found tracked read %s in output candidate cluster %s' % (args.trackread, cluster.uuid))
                        tracking = True

                    if targets is not None:
                        in_targets = False
                        
                        for rec in targets[cluster.chrom()].find(int(cluster.breakpoints[0]), int(cluster.breakpoints[1])):
                            in_targets = True

                        if not in_targets:
                            logger.debug('cluster %s (%d of %d) skipped, not in --targets' % (cluster.uuid, c_tot, len(processed_clusters)))
                            c_tot += 1

                            if tracking:
                                logger.debug('tracked read %s in dropped cluster %s (--targets)' % (args.trackread, cluster.uuid))
                            continue

                    logger.debug('start output processing of cluster %s (%d of %d)' % (cluster.uuid, c_tot, len(processed_clusters)))

                    nr = 'NA'

                    if nonref:
                        logger.debug('nonref check cluster %s' % cluster.uuid)
                        ins_start, ins_end = cluster.interval()

                        if ins_start < 0:
                            ins_start = 0

                        if ins_end < 0:
                            ins_end = 0

                        if cluster.chrom() in nonref.contigs:
                            for nr in nonref.fetch(cluster.chrom(), ins_start, ins_end):
                                nr = '_'.join(nr.strip().split())

                    logger.debug('sorted_unmapped_segments cluster %s' % cluster.uuid)
                    c_unmap = cluster.sorted_unmapped_segments()[0]

                    start_te    = 'NA'
                    end_te      = 'NA'
                    start_unmap = 'NA'
                    end_unmap   = 'NA'
                    unmap_cover = 'NA'

                    if cluster.cons_te_align is not None:
                        start_te = cluster.cons_te_align.min_qry_coord()
                        end_te = cluster.cons_te_align.max_qry_coord()
                        start_unmap = cluster.cons_te_align.min_tgt_coord()
                        end_unmap = cluster.cons_te_align.max_tgt_coord()
                        unmap_cover = (end_unmap-start_unmap)/(c_unmap[1] - c_unmap[0])

                    output = od()

                    output['UUID']        = cluster.uuid
                    output['Chrom']       = cluster.chrom()
                    output['Start']       = int(cluster.breakpoints[0])
                    output['End']         = int(cluster.breakpoints[1])
                    output['Strand']      = cluster.te_orientation()
                    output['Family']      = cluster.te_major_family()
                    output['Subfamily']   = cluster.te_major_subfam()
                    output['StartTE']     = start_te
                    output['EndTE']       = end_te
                    output['LengthIns']   = c_unmap[1] - c_unmap[0]
                    output['Inversion']   = cluster.detect_inversion()
                    output['UnmapCover']  = unmap_cover
                    output['MedianMapQ']  = cluster.te_median_mapq()
                    output['TEMatch']     = cluster.te_match()
                    output['UsedReads']   = cluster.te_useable_count()
                    output['SpanReads']   = cluster.te_embedded_count()
                    output['NumSamples']  = cluster.te_samples()
                    output['SampleReads'] = cluster.te_sample_count()
                    output['EmptyReads']  = cluster.empty_reads_count()
                    output['EmptyPhase']  = cluster.empty_phase()
                    output['NonRef']      = nr
                    output['TSD']         = cluster.tsd_seq
                    output['Consensus']   = cluster.cons
                    output['Phasing']     = cluster.phase()
                    output['Remappable']  = cluster.breakpoints_remappable
                    output['Filter']      = assign_filter(output, c_unmap, args, noref_mode, cluster_forest)

                    detail = od()

                    if args.detail_output:
                        with open(outbase+'/'+cluster.uuid+'.detail.out', 'w') as detail_out:
                            detail_out.write('%s\n' % '\t'.join(detail_header))

                            for insread in cluster.reads:
                                detail['Cluster']    = cluster.uuid
                                detail['BamName']    = insread.bamname
                                detail['ReadName']   = insread.r_name
                                detail['IsSpanRead'] = insread.is_ins
                                detail['TEAlign'] = 'NA'

                                if insread.te:
                                    detail['TEAlign'] = '|'.join([','.join(a.original) for a in insread.te.alignments])

                                detail['TEOverlap'] = insread.te_overlap_frac()
                                detail['Useable'] = insread.useable
                                detail['RefPos'] = insread.r_pos
                                detail['Phase'] = insread.phase

                                detail_out.write('%s\n' % '\t'.join(map(str, [detail[col] for col in detail_header])))

                    if args.somatic:
                        output['Somatic'] = predict_somatic(output)

                    if args.trdcol:
                        trd_5p = 'NA'
                        trd_3p = 'NA'

                        if cluster.cons_te_align is not None:
                            ins_seg = cluster.cons[c_unmap[0]:c_unmap[1]]
                            trd_5p = ins_seg[:start_unmap]
                            trd_3p = ins_seg[end_unmap:]

                        if output['Strand'] == '-':
                            trd_5p, trd_3p = trd_3p, trd_5p

                        if trd_5p == '':
                            trd_5p = 'NA'

                        if trd_3p == '':
                            trd_3p = 'NA'

                        output['Transduction_5p'] = trd_5p
                        output['Transduction_3p'] = trd_3p

                    if args.color_consensus:
                        C_YELLOW  = "\033[33m"
                        C_BLUE    = "\033[34m"
                        C_PINK    = "\033[91m"
                        C_END     = "\033[0m"

                        col_seg = cluster.cons[c_unmap[0]:c_unmap[1]]

                        if cluster.cons_te_align is not None:
                            col_seg=col_seg[:start_unmap] + C_BLUE + col_seg[start_unmap:end_unmap] + C_YELLOW + col_seg[end_unmap:]

                        tsd_len = 0
                        if output['TSD'] != 'NA':
                            tsd_len = len(output['TSD'])

                        tsd_1 = C_PINK + cluster.cons[c_unmap[0]-tsd_len:c_unmap[0]] + C_END
                        tsd_2 = C_PINK + cluster.cons[c_unmap[1]:c_unmap[1]+tsd_len] + C_END

                        output['Consensus'] = cluster.cons[:c_unmap[0]-tsd_len]+tsd_1+C_YELLOW+col_seg+C_END+tsd_2+cluster.cons[c_unmap[1]+tsd_len:]

                    output_recs.append(output)

                    logger.debug('finish output cluster %d of %d' % (c_tot, len(processed_clusters)))

                    c_count += 1
                    c_tot += 1

                else:
                    if cluster is None:
                        logger.debug('skipped empty cluster (%d of %d)' % (c_tot, len(processed_clusters)))
                    else:
                        logger.debug('skipped unusable cluster %s (%d of %d)' % (cluster.uuid, c_tot, len(processed_clusters)))
                        if args.trackread and args.trackread in [read.r_name for read in cluster.reads]:
                            logger.debug('tracked read %s was in unusable cluster %s (no consensus)' % (args.trackread, cluster.uuid))
                    c_tot += 1

            logger.info('finished %s. wrote %d records to %s' % (pickle_fn, c_count, outbase+'.table.txt'))

            if args.keep_pickles or args.use_pickles:
                logger.info('kept .pickle file %s' % pickle_fn)
            else:
                os.remove(pickle_fn)

    output_table(args, output_recs, header, outbase)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Find TE insertions in ONT data')

    __version__ = "1.3.0"
    parser.add_argument('-v', '--version', action='version', version='%(prog)s {version}'.format(version=__version__))

    parser.add_argument('-b', '--bams', required=True, help='comma seperated bam list')
    parser.add_argument('-e', '--elts', help='reference elements .fa or "none" for no TE ref mode', required=True)
    parser.add_argument('-r', '--ref', help='reference fasta (samtools faidx indexed)', required=True)
    parser.add_argument('-p', '--procs', help='process count', default=1)
    parser.add_argument('-m', '--minreads', default=3, help='minimum read count to form cluster (default = 3)')
    parser.add_argument('--embed_minreads', default=1, help='minimum number of reads completely embedding insertion (default = 1)')
    parser.add_argument('-o', '--outbase', default=None, help='base name for output files (defaults to bam name(s)')
    parser.add_argument('-c', '--chroms', default=None, help='limit to chromsomes in file')
    parser.add_argument('--targets', default=None, help='focus only on targets in file (BED-3)')
    parser.add_argument('--max_te_len', default=10000, help='maximum insertion size (default = 10000)')
    parser.add_argument('--min_te_len', default=200, help='minimum insertion size (default = 200)')
    parser.add_argument('--min_alt_frac', default=0.5, help='parameter for base changes in consensus cleanup (defult=0.5)')
    parser.add_argument('--min_alt_depth', default=3, help='parameter for base changes in consensus cleanup (defult=3)')
    parser.add_argument('--min_total_depth_frac', default=0.1, help='parameter for base changes in consensus cleanup (defult=0.25)')
    parser.add_argument('--max_cluster_size', default=None, help='limit cluster size, embed-biased downsample larger clusters')
    parser.add_argument('--wiggle', default=200, help='interval padding for breakpoint search (default = 50)')
    parser.add_argument('--flanksize', default=500, help='flank size for read trimming (default = 500)')
    parser.add_argument('-n', '--nonref', help='known nonref tabix', default=None)
    parser.add_argument('--debug', action='store_true', default=False)
    parser.add_argument('--trackread', default=None, help='debugging function')
    parser.add_argument('--color_consensus', action='store_true', default=False)
    parser.add_argument('--detail_output', action='store_true', default=False, help='outputs per-insertion .bams and per-read mapping information')
    parser.add_argument('--extend_consensus', default=0, help='if --detail_output option is enabled, extend output per-sample consensus by n bases (default 0)')
    parser.add_argument('--trdcol', action='store_true', default=False, help='add columns needed for transduction mapping')
    parser.add_argument('--somatic', action='store_true', default=False, help='predict somatic from phase information, turns on --strict')
    parser.add_argument('--strict', action='store_true', default=False, help='additional filters')
    parser.add_argument('--keep_pickles', action='store_true', default=False, help='do not remove per-chromosome pickle files')
    parser.add_argument('--use_pickles', default=None, help='use .pickle files in specified folder instead of building from scratch')
    parser.add_argument('--methylartist', action='store_true', help='generate methylartist commands, must be used with --detail_out')
    args = parser.parse_args()
    main(args)
